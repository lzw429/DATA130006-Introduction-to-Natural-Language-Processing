%! Author = Yiheng Shu
%! Date = 2019-08-01

\documentclass{article}
\usepackage{authblk}
% \usepackage[UTF8]{ctex}

\title{Spelling Correction}
\author[*]{Yiheng Shu}
\affil[*]{Software College, Northeastern University, China}
\date{July 2019}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}

    \maketitle

    \section{Introduction}\label{sec:introduction}
    This report is for \textit{Homework 1: Spelling Correction} of \textit{DATA130006: Introduction to Natural Language Processing}.

    The project attempted to correct the spelling of a piece of English news text.
    The types of spelling errors are mainly divided into \textbf{non-word errors} and \textbf{real word errors}.

    For non-word errors, the words that need to be checked are compared to all words in the vocabulary, and words that are not in the vocabulary must be non-word errors.
    The rest of the words appear in the vocabulary, which may be real word errors or correct words.

    The algorithms involved in this project are mainly the noisy-channel model and the language model.

    \section{Algorithm Flow}
    \label{sec:algorithm-flow}

    From the perspective of code implementation, the flow of program execution is as follows.

    1) Preprocess.
    The program downloads nltk Reuters corpus and count the corpus for n-gram.
    In the program, preprocessing involves loading the confusion matrix and vocabulary from an external files.

    2) Noisy channel model.

    3) N-gram language model. Considering both the model effect and the computational performance, the program uses the Bigram model, which is a form of n = 2 in the n-gram model. Bigrams help provide the conditional probability of a token given the preceding token, when the relation of the conditional probability is applied:
    $$
    P(W_n | W_{n-1}) = \frac{P(W_{n-1}, W_n)}{P(W_{n-1})}
    $$

    \section{Implementation Details}
    \label{sec:implementation-details}

    In the process of algorithm implementation, there are some details that need to be considered.
    These problems may not be considered in the design of the algorithm, but they occur during program debugging.

    \subsection{The Integrity of Vocabulary}
    \label{subsec:the-integrity-of-vocabulary}

    The first is the integrity of the thesaurus given by the title.
    For checking for non-word errors, the larger the lexicon, the better.
    The lexicon given by the title has a total of more than 48,000 lines.
    Excluding the repetition of uppercase and lowercase words, it actually contains more than 20,000 common words.

    In natural language, vocabulary does not always appear as a root.
    Nouns may be plural, and verbs may be in the form of third-person singular forms, past participles, and present participles.
    Each word may be followed by punctuation marks such as commas, periods, and question marks, as well as suffixes required for grammars such as tenses and possessives.

    The given vocabulary includes nouns with capitalized initials, nouns in plural, and third-person singular forms of verbs.
    When the program judges a non-word error, it first removes the suffix of the word and converts it into the form contained in the vocabulary.

    Some common words are not included in the vocabulary, such as \textit{won't}, \textit{don't} and \textit{can't}, so they are manually added to the vocabulary.
    
    \subsection{Confusion Matrix}
    \label{subsec:confusion-matrix}
    
    A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one.
    This program directly uses the confusion matrix provided by \cite{kernighan1990spelling}.
    The four confusion matrices are (1) del[x,y], the number of times that the characters xy (in the correct word) were typed as x in the training set, (2), add[x,y], the number of times that x was typed as xy, (3) sub[x,y], the number of times that y was typed as x, and (4) rev[x,y], the number of times that xy was typed as yx.

    \subsection{Edit Distance Implementation}
    \label{subsec:edit-distance-implementation}

    In this task, the Damerau Levenshtein distance is used to calculate the minimum number of operations for a word to be modified to another word.
    The types of operations include insertion, deletion, single-character substitution, and transposition of two adjacent characters.
    After discovering that the edit distance calculation function directly implemented in Python (without speeding up tools such as numpy) is inefficient, the program uses \textit{pyxDamerauLevenshtein} package of PyPI to calculate the edit distance.
    It implements algorithm for Python in Cython for high performance, and it runs in $O(N*M)$ time using $O(M)$ space.


\bibliographystyle{plain}
\bibliography{references}

\end{document}
